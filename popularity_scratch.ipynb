{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169e3bb-0334-4f44-a03c-6108f41a0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec5737-002c-4c17-8c67-d28cd8fc9a45",
   "metadata": {},
   "source": [
    "-Need to do a sort of sentiment analysis like with word vectorization\n",
    "-Load in other data things\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce9f6a2-21de-47f7-b6b1-a08ccafb17bf",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/mehmetlaudatekman/tutorial-word-embeddings-with-svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee065d-659b-48a1-ad96-bb36a6d655fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shower_dp = 'data/showerThoughts.csv'\n",
    "shower_data = pd.read_csv(shower_dp,header=0)\n",
    "shower_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf8d3c1-1909-478e-afae-522b35223bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "shower_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f2380-2349-4b52-b197-c426697f5c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shower_data = shower_data.drop(['domain', 'id','permalink', 'over_18','link_flair_css_class',\n",
    "       'author_flair_css_class', 'selftext','thumbnail', 'name', 'url', 'link_flair_text','distinguished'], axis='columns')\n",
    "shower_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c58407-27cc-4c09-836f-2f0f20a036a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(shower_data['ups'], shower_data['downs'],color=\"r\")\n",
    "plt.xlabel(\"Up Votes\")\n",
    "plt.ylabel(\"Down Votes\")\n",
    "plt.title(\"Engagement\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db521c6-16c6-4a58-80ce-9f69f7f9d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "shower_data[\"engagement\"] = shower_data[\"ups\"] + shower_data[\"downs\"] + shower_data[\"num_comments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb12f6a-27b3-4592-aca5-dc510b6c1ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#August 20th 2013 date created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675a8d8-cb5b-4a5e-9408-6dbd6a1b2cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shower_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f89e04-8317-4175-9c9e-e2532a4fb026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247bf9a-629b-4f26-99c6-ed9ed652742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(['punkt', \"names\", \"stopwords\", \"averaged_perceptron_tagger\", \"vader_lexicon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533fa152-7aa3-4652-a758-72cf2fad44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = shower_data['title']\n",
    "tokens = trial.apply(lambda x: word_tokenize(x))\n",
    "#tokenizing all of the 'title' column, need to now vectorize them// word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053e10a-26be-4852-b6e3-270f54b5462f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords.append(\".\")\n",
    "stopwords.append(\",\")\n",
    "\n",
    "#Cleaning out stopwords\n",
    "clean = []\n",
    "for sent in tokens:\n",
    "    new_sent = []\n",
    "    for word in sent:\n",
    "        if word.lower() not in stopwords:\n",
    "            new_sent.append(word)\n",
    "    clean.append(new_sent)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e822d-0421-4b02-8e91-2029ff58727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_words = get_all_words(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c012ac3e-b24e-4ac8-8a1b-8f774c30d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(all_words)\n",
    "print(freq_dist_pos.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b81a284-e2b5-4586-bc1a-3bca544a1b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.sentiment \n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e6b7b9-96af-453e-b74b-37c3d8f00365",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fe0776-13d9-4807-b078-2d9d7d78344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_to_df(titles):\n",
    "    neg = []\n",
    "    neu = []\n",
    "    pos = []\n",
    "    for t in titles:\n",
    "        scores = sia.polarity_scores(t)\n",
    "        neg.append(scores['neg'])\n",
    "        neu.append(scores['neu'])\n",
    "        pos.append(scores['pos'])\n",
    "        \n",
    "    return neg, neu, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc283983-2e41-4685-a37b-dd45a787295f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neg, neu, pos = sentiment_to_df(shower_data['title'])\n",
    "shower_data['neg'] = neg\n",
    "shower_data['neu'] = neu\n",
    "shower_data['pos'] = pos\n",
    "shower_data\n",
    "#TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640095f9-ab81-47bf-98cc-d2b3f8680537",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = shower_data[['neg', 'neu', 'pos']]\n",
    "targets = shower_data['ups']\n",
    "\n",
    "features_norm = (features-features.min())/(features.max()-features.min())\n",
    "targets_norm = (targets-targets.min())/(targets.max()-targets.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59dee1-75b2-4416-8364-23bfb86f9a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c2477a-9590-41ce-9328-726cfb55d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(features)\n",
    "feat = scaler.transform(features)\n",
    "feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24ef37-d761-4d75-8c01-3091019ace38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_scaler = MinMaxScaler()\n",
    "t = np.array(targets).reshape(-1, 1)\n",
    "target_scaler.fit(t)\n",
    "\n",
    "targ = target_scaler.transform(t)\n",
    "targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10bd221-32a4-46df-a751-adb341c7f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "features_poly = poly.fit_transform(features_norm)\n",
    "f_poly = poly.fit_transform(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd37d539-cab3-4b01-a4bd-0b84703ae3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ce43f-3203-4036-bdd9-08367983868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(f_poly, targ, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8913b65-6641-4f48-9fff-377c2ae2400c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68771153-b4dc-4011-ac81-2831a6e7becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "reg = linear_model.BayesianRidge()\n",
    "reg.fit(X_train, y_train)\n",
    "preds = reg.predict(X_test)\n",
    "mean_squared_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae04c2-8fbe-4efb-82fc-c1b11a7efe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3f3db-0ab1-4fc3-91b7-bfb018fd3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#features.iloc[0].reshape(1, -1)\n",
    "x = np.array(features.iloc[996].tolist()).reshape(1, -1)\n",
    "x_poly = poly.fit_transform(x)\n",
    "x_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a24b35-410b-40c0-acbc-75ac7e969916",
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = reg.predict(x_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab5b8c4-b5ec-4fe8-a43f-07ca698a017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656bc8a-efb0-401c-a90c-be7d629f9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scaler.inverse_transform([fun])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ebb4f-dc4f-477d-9b64-6f962160d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca824997-9128-449c-b228-40c88ac141ca",
   "metadata": {},
   "outputs": [],
   "source": [
    " # config values// Beginning of word embeddings\n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487ff69-5b7e-4c65-bf0a-f46d40209bab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = shower_data['title']\n",
    "targets_ups = shower_data['ups']\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fafea-54bc-464d-880d-e1125706a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(clean, targ, test_size=.2, train_size=.8, random_state=None, shuffle=True, stratify=None)\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e78f3a0-e849-4673-8dba-3c5a5f592143",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fae24d-bba4-4b0e-b4dd-29b5db12e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.BayesianRidge()\n",
    "reg.fit(X_train, y_train)\n",
    "preds = reg.predict(X_test)\n",
    "mean_squared_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3482e492-bf36-4bb6-9cf9-78fcac6aa1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shower_data['ups'].describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d54d6e-ad3e-477f-aeb1-cc96f35a47e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
